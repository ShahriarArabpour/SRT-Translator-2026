1
00:00:00,000 --> 00:00:01,920
today I'll show you how to implement

2
00:00:01,920 --> 00:00:05,040
chat with your document system in just

3
00:00:05,040 --> 00:00:08,340
four lines of code I recently started

4
00:00:08,340 --> 00:00:11,700
exploring llama index which is an

5
00:00:11,700 --> 00:00:14,519
alternative to link chain just like link

6
00:00:14,519 --> 00:00:17,520
chain Lama index gives us the ability to

7
00:00:17,520 --> 00:00:19,500
build powerful applications based on

8
00:00:19,500 --> 00:00:22,080
large language models these applications

9
00:00:22,080 --> 00:00:24,180
include document q a

10
00:00:24,180 --> 00:00:27,060
data augmented chat box knowledge agents

11
00:00:27,060 --> 00:00:28,800
the great part is you can connect

12
00:00:28,800 --> 00:00:30,660
different types of data sources

13
00:00:30,660 --> 00:00:33,600
including structured and structured and

14
00:00:33,600 --> 00:00:36,360
semi-structured data sources so using

15
00:00:36,360 --> 00:00:38,940
lamb index you can quickly build llm

16
00:00:38,940 --> 00:00:41,579
based applications just like Lang chain

17
00:00:41,579 --> 00:00:44,399
the main reason for me of exploring Lama

18
00:00:44,399 --> 00:00:46,980
index was the ability to fine-tune

19
00:00:46,980 --> 00:00:49,140
embedding models to improve the

20
00:00:49,140 --> 00:00:51,660
performance of document Q and A systems

21
00:00:51,660 --> 00:00:54,360
using large language models so basically

22
00:00:54,360 --> 00:00:56,699
that means you can not only find during

23
00:00:56,699 --> 00:00:58,559
the large language model on your data

24
00:00:58,559 --> 00:01:01,079
set but you can also fine tune their

25
00:01:01,079 --> 00:01:03,480
betting model and that will improve the

26
00:01:03,480 --> 00:01:05,400
performance of your recruited system

27
00:01:05,400 --> 00:01:08,880
I'll cover this topic in a future video

28
00:01:08,880 --> 00:01:11,820
in this first video I'll show you how to

29
00:01:11,820 --> 00:01:13,979
build a chat with your document system

30
00:01:13,979 --> 00:01:17,220
using Lama index you probably have seen

31
00:01:17,220 --> 00:01:19,200
this diagram before so there are

32
00:01:19,200 --> 00:01:21,659
different components the first component

33
00:01:21,659 --> 00:01:25,320
is basically loading your documents then

34
00:01:25,320 --> 00:01:27,540
you want to divide your documents into

35
00:01:27,540 --> 00:01:30,659
smaller chunks with a predefined chunk

36
00:01:30,659 --> 00:01:33,479
size so each of the chunks you compute

37
00:01:33,479 --> 00:01:36,000
different embeddings these embeddings

38
00:01:36,000 --> 00:01:38,579
are numerical representations of the

39
00:01:38,579 --> 00:01:41,340
text contained in a chunk then you

40
00:01:41,340 --> 00:01:45,000
create a semantic index so basically

41
00:01:45,000 --> 00:01:48,119
this is your vector strap after that you

42
00:01:48,119 --> 00:01:50,460
can actually use the swipe to store to

43
00:01:50,460 --> 00:01:53,159
chat with your documents now in order to

44
00:01:53,159 --> 00:01:54,840
chat with your document

45
00:01:54,840 --> 00:01:57,540
the system takes your questions compute

46
00:01:57,540 --> 00:01:59,939
the same embedding as the embedding it

47
00:01:59,939 --> 00:02:02,640
computed for the chunks then performed a

48
00:02:02,640 --> 00:02:04,439
semantic search on your knowledge base

49
00:02:04,439 --> 00:02:05,880
so

50
00:02:05,880 --> 00:02:08,099
as a result of this search it will

51
00:02:08,099 --> 00:02:09,300
return

52
00:02:09,300 --> 00:02:11,280
a certain number of chunks that are

53
00:02:11,280 --> 00:02:13,739
relevant to your question so let's say

54
00:02:13,739 --> 00:02:16,140
by default number index returns only two

55
00:02:16,140 --> 00:02:18,000
chunks out of all the chunks available

56
00:02:18,000 --> 00:02:20,400
in the knowledge base

57
00:02:20,400 --> 00:02:22,620
and these return chunks will be used as

58
00:02:22,620 --> 00:02:26,099
a context to the llm of your choosing so

59
00:02:26,099 --> 00:02:29,040
the llm gets the question from the user

60
00:02:29,040 --> 00:02:31,440
along with the context that was written

61
00:02:31,440 --> 00:02:33,780
by the embedding model during the

62
00:02:33,780 --> 00:02:36,420
semantic search and the llm will

63
00:02:36,420 --> 00:02:38,099
generate a response and that is going to

64
00:02:38,099 --> 00:02:41,099
be shown to the user as you can see this

65
00:02:41,099 --> 00:02:43,200
whole process can be divided into four

66
00:02:43,200 --> 00:02:46,379
different steps and I'll show you how to

67
00:02:46,379 --> 00:02:48,959
implement this whole process using just

68
00:02:48,959 --> 00:02:52,140
four lines of codes within gamma index

69
00:02:52,140 --> 00:02:54,540
okay so let's start looking at the code

70
00:02:54,540 --> 00:02:57,840
so I'm using Google Chrome in order to

71
00:02:57,840 --> 00:03:00,660
the notebook and the code so first we're

72
00:03:00,660 --> 00:03:02,700
installing all the required packages so

73
00:03:02,700 --> 00:03:05,940
we are installing lamba index then open

74
00:03:05,940 --> 00:03:08,459
AI so for the initial example I will

75
00:03:08,459 --> 00:03:11,340
show you how to implement the document q

76
00:03:11,340 --> 00:03:14,879
a using open a llm and open AI

77
00:03:14,879 --> 00:03:17,340
embeddings but I'll also show you how to

78
00:03:17,340 --> 00:03:20,940
replace the openai llm within open

79
00:03:20,940 --> 00:03:23,340
source llm from hanging face

80
00:03:23,340 --> 00:03:25,680
and for that we will need Transformers

81
00:03:25,680 --> 00:03:28,280
package to access the hug interface

82
00:03:28,280 --> 00:03:32,159
based llms and the accelerate package in

83
00:03:32,159 --> 00:03:34,379
order to accelerate the processing or

84
00:03:34,379 --> 00:03:37,140
speed of running local llms

85
00:03:37,140 --> 00:03:40,080
now for the initial example we will need

86
00:03:40,080 --> 00:03:43,260
the open AI API key in order to access

87
00:03:43,260 --> 00:03:46,379
open AIS embeddings and llm so you'll

88
00:03:46,379 --> 00:03:48,000
need to provide those

89
00:03:48,000 --> 00:03:50,760
okay next we need to import the required

90
00:03:50,760 --> 00:03:53,280
packages so we are importing open AI

91
00:03:53,280 --> 00:03:56,099
from lamba index then we are inputting

92
00:03:56,099 --> 00:03:58,920
two auxiliary functions

93
00:03:58,920 --> 00:04:01,980
or objects one is e Vector store index

94
00:04:01,980 --> 00:04:04,799
and the other one is a simple directory

95
00:04:04,799 --> 00:04:08,159
reader the use will become clearer when

96
00:04:08,159 --> 00:04:10,860
we look at the code and then we are also

97
00:04:10,860 --> 00:04:15,480
importing some packages just to properly

98
00:04:15,480 --> 00:04:18,780
format the output from the model now in

99
00:04:18,780 --> 00:04:21,299
order to chat with your documents you

100
00:04:21,299 --> 00:04:22,800
will first need access to those

101
00:04:22,800 --> 00:04:25,139
documents so the way I'm going to do it

102
00:04:25,139 --> 00:04:28,020
is I will create a folder called Data

103
00:04:28,020 --> 00:04:29,940
and within this folder I will upload

104
00:04:29,940 --> 00:04:34,440
this file now this specific essay is

105
00:04:34,440 --> 00:04:37,020
titled what I worked on and this is

106
00:04:37,020 --> 00:04:39,120
basically an essay written by allgram

107
00:04:39,120 --> 00:04:42,720
when he was working with y combinators

108
00:04:42,720 --> 00:04:45,540
so we're going to be using this as an

109
00:04:45,540 --> 00:04:48,660
example data set but you can upload any

110
00:04:48,660 --> 00:04:51,720
type of documents for example text Word

111
00:04:51,720 --> 00:04:55,259
documents or PDFs now going back to the

112
00:04:55,259 --> 00:04:57,780
architecture first we will need to load

113
00:04:57,780 --> 00:05:00,840
the documents now Lama index provides a

114
00:05:00,840 --> 00:05:03,240
very simple functionality to do that so

115
00:05:03,240 --> 00:05:04,979
there's this class called Simple

116
00:05:04,979 --> 00:05:07,020
directory reader you simply need to

117
00:05:07,020 --> 00:05:10,380
provide the name of the folder or path

118
00:05:10,380 --> 00:05:13,080
of the folder so in this case our folder

119
00:05:13,080 --> 00:05:15,120
is called data and that's why I provided

120
00:05:15,120 --> 00:05:17,580
the name data then we need to call the

121
00:05:17,580 --> 00:05:20,100
load data function on top of that and

122
00:05:20,100 --> 00:05:22,500
that will load our documents

123
00:05:22,500 --> 00:05:24,960
now the great thing about this is you

124
00:05:24,960 --> 00:05:26,759
can have different type of documents

125
00:05:26,759 --> 00:05:29,940
within the folder and this class will

126
00:05:29,940 --> 00:05:32,580
Define which loader to use with those

127
00:05:32,580 --> 00:05:33,720
documents

128
00:05:33,720 --> 00:05:36,180
and we can also look at it this is

129
00:05:36,180 --> 00:05:38,400
basically the text file and the whole

130
00:05:38,400 --> 00:05:40,680
text file is loaded in here

131
00:05:40,680 --> 00:05:43,560
now the next step in our architecture is

132
00:05:43,560 --> 00:05:45,900
to create a vector store so this

133
00:05:45,900 --> 00:05:48,180
involves dividing your documents into

134
00:05:48,180 --> 00:05:51,000
chunks Computing their beddings and then

135
00:05:51,000 --> 00:05:53,520
storing both the embeddings as well as

136
00:05:53,520 --> 00:05:55,440
the chunks in a vector store

137
00:05:55,440 --> 00:05:58,080
again the number index makes this very

138
00:05:58,080 --> 00:06:00,600
easy we're going to use the vector store

139
00:06:00,600 --> 00:06:03,300
index class then we are going to be

140
00:06:03,300 --> 00:06:05,460
creating the vector store from documents

141
00:06:05,460 --> 00:06:08,699
we'll pass on our documents in here and

142
00:06:08,699 --> 00:06:12,240
this will create a vector store index

143
00:06:12,240 --> 00:06:13,259
for us

144
00:06:13,259 --> 00:06:16,320
okay so we just created our index now

145
00:06:16,320 --> 00:06:18,360
next we are going to look at how to

146
00:06:18,360 --> 00:06:20,660
customize different options for example

147
00:06:20,660 --> 00:06:23,280
the chunk size the type of embeddings

148
00:06:23,280 --> 00:06:26,039
and everything but before that we just

149
00:06:26,039 --> 00:06:28,680
want to ask questions so for that we

150
00:06:28,680 --> 00:06:31,440
need to create a query engine

151
00:06:31,440 --> 00:06:34,699
now I find this to be very streamlined

152
00:06:34,699 --> 00:06:38,479
in Nama index compared to

153
00:06:38,479 --> 00:06:41,900
so here basically you can

154
00:06:41,900 --> 00:06:45,840
create a query engine on the index by

155
00:06:45,840 --> 00:06:49,259
using the as query engine now

156
00:06:49,259 --> 00:06:51,539
if you want to chat with your documents

157
00:06:51,539 --> 00:06:54,900
and want to have memory so you were

158
00:06:54,900 --> 00:06:57,080
going to use a function called as

159
00:06:57,080 --> 00:07:00,000
chatbot and that will basically enable

160
00:07:00,000 --> 00:07:03,120
memory but as a query engine you don't

161
00:07:03,120 --> 00:07:05,819
have the memory component in here now in

162
00:07:05,819 --> 00:07:08,880
order to use this pretty engine to get a

163
00:07:08,880 --> 00:07:11,280
response based on the user question so

164
00:07:11,280 --> 00:07:12,900
it has to go through this process that

165
00:07:12,900 --> 00:07:14,699
it needs to compute embeddings for the

166
00:07:14,699 --> 00:07:17,280
question then do the semantic search and

167
00:07:17,280 --> 00:07:19,979
use the returned chunks as context with

168
00:07:19,979 --> 00:07:21,960
the lln along with the question to get

169
00:07:21,960 --> 00:07:24,780
to generate an answer all of this is an

170
00:07:24,780 --> 00:07:27,240
implemented using a single line of code

171
00:07:27,240 --> 00:07:30,300
but basically what you do is you use the

172
00:07:30,300 --> 00:07:32,639
query engine then call this function

173
00:07:32,639 --> 00:07:35,940
called query and you pass on your

174
00:07:35,940 --> 00:07:38,759
question to this function so for example

175
00:07:38,759 --> 00:07:41,280
the question that we are asking is what

176
00:07:41,280 --> 00:07:44,699
did the author do growing up and we will

177
00:07:44,699 --> 00:07:47,580
get a response in here now the response

178
00:07:47,580 --> 00:07:49,259
that you get has a lot of different

179
00:07:49,259 --> 00:07:51,960
components in care but we're simply

180
00:07:51,960 --> 00:07:54,180
interested in in the answer

181
00:07:54,180 --> 00:07:57,300
I'll show you what all this text means

182
00:07:57,300 --> 00:08:00,120
in a bit but first let me show you how

183
00:08:00,120 --> 00:08:03,120
to get the answer from the llm for that

184
00:08:03,120 --> 00:08:05,180
we will use the display function from

185
00:08:05,180 --> 00:08:08,819
IPython and put the response in markdown

186
00:08:08,819 --> 00:08:11,220
and also bold it so here's the answer

187
00:08:11,220 --> 00:08:13,440
from the model the author worked on

188
00:08:13,440 --> 00:08:15,120
writing and programming outside of the

189
00:08:15,120 --> 00:08:17,520
school before College they wrote short

190
00:08:17,520 --> 00:08:20,520
stories and tried writing programs on an

191
00:08:20,520 --> 00:08:23,819
IBM 1401 computer using an early version

192
00:08:23,819 --> 00:08:26,039
of quadrant right so this is the answer

193
00:08:26,039 --> 00:08:29,400
that was generated by the llm using the

194
00:08:29,400 --> 00:08:32,000
context provided by the embedding model

195
00:08:32,000 --> 00:08:34,459
as you can see

196
00:08:34,459 --> 00:08:37,680
apart from the Imports we were able to

197
00:08:37,680 --> 00:08:41,279
build a document q a system using just

198
00:08:41,279 --> 00:08:42,839
all lines of codes

199
00:08:42,839 --> 00:08:46,320
and this implementation is very clean

200
00:08:46,320 --> 00:08:48,899
next I want to show you how to customize

201
00:08:48,899 --> 00:08:51,300
different parts of this diagram for

202
00:08:51,300 --> 00:08:53,700
example how do you define the chunk size

203
00:08:53,700 --> 00:08:56,580
chunk or left or how do you change the

204
00:08:56,580 --> 00:08:59,160
llm and so on and so forth but before

205
00:08:59,160 --> 00:09:01,500
that let's look at some very important

206
00:09:01,500 --> 00:09:03,480
Concepts when it comes to the vector

207
00:09:03,480 --> 00:09:06,240
stores so right now the index that we

208
00:09:06,240 --> 00:09:09,779
created is just in memory but you can

209
00:09:09,779 --> 00:09:12,420
persist this by calling this function

210
00:09:12,420 --> 00:09:15,420
persist on the index that you created

211
00:09:15,420 --> 00:09:18,300
now the way it's going to work is if we

212
00:09:18,300 --> 00:09:21,240
run this so by default it's going to

213
00:09:21,240 --> 00:09:24,240
create a folder called storage and

214
00:09:24,240 --> 00:09:27,180
within this you will see multiple Json

215
00:09:27,180 --> 00:09:30,360
files now this functionality is critical

216
00:09:30,360 --> 00:09:32,880
because it enables you to create the

217
00:09:32,880 --> 00:09:33,899
index one

218
00:09:33,899 --> 00:09:36,440
install it on your

219
00:09:36,440 --> 00:09:41,160
disk and then use it in your future runs

220
00:09:41,160 --> 00:09:43,680
right now in order to load a vector

221
00:09:43,680 --> 00:09:46,440
store that was stored on the disk we're

222
00:09:46,440 --> 00:09:48,480
going to be using the storage context

223
00:09:48,480 --> 00:09:51,660
along with the load index from Storage

224
00:09:51,660 --> 00:09:53,279
class

225
00:09:53,279 --> 00:09:55,320
now the way it works is that you will

226
00:09:55,320 --> 00:09:58,320
use a storage context to read the

227
00:09:58,320 --> 00:10:01,680
content of the index and then from the

228
00:10:01,680 --> 00:10:03,420
storage context that you just created

229
00:10:03,420 --> 00:10:06,120
you are going to recreate the index

230
00:10:06,120 --> 00:10:08,459
right and then you can use it exactly

231
00:10:08,459 --> 00:10:10,680
the way we did it before

232
00:10:10,680 --> 00:10:13,019
now I hope this is clear another

233
00:10:13,019 --> 00:10:16,200
question that comes in mind is what

234
00:10:16,200 --> 00:10:18,660
exactly is in this Vector store

235
00:10:18,660 --> 00:10:20,640
now in order to show you I'm going to

236
00:10:20,640 --> 00:10:23,880
open all these four files and the two

237
00:10:23,880 --> 00:10:25,500
important files that we want to look at

238
00:10:25,500 --> 00:10:28,980
is the stock stored and the vector Stone

239
00:10:28,980 --> 00:10:31,500
so the vector store as you can see here

240
00:10:31,500 --> 00:10:33,720
is basically the embeddings that we

241
00:10:33,720 --> 00:10:37,019
computed for each of the chunk and

242
00:10:37,019 --> 00:10:39,720
depending on the embedding model that

243
00:10:39,720 --> 00:10:41,640
you choose you are going to get

244
00:10:41,640 --> 00:10:44,579
different number of embeddings per chunk

245
00:10:44,579 --> 00:10:46,680
now similar to the vector store for

246
00:10:46,680 --> 00:10:49,920
embeddings there is a doc store so here

247
00:10:49,920 --> 00:10:53,040
you will see different chunks from the

248
00:10:53,040 --> 00:10:55,440
document so basically this is text that

249
00:10:55,440 --> 00:10:58,140
is to add in two different chunks and

250
00:10:58,140 --> 00:11:01,140
the third most important thing is index

251
00:11:01,140 --> 00:11:04,620
store basically this has the hash or

252
00:11:04,620 --> 00:11:08,279
address for different chunks and using

253
00:11:08,279 --> 00:11:11,519
this index tool it determines which

254
00:11:11,519 --> 00:11:14,579
embeddings in here belongs to which

255
00:11:14,579 --> 00:11:15,959
chunk

256
00:11:15,959 --> 00:11:20,040
now when you create in a vector store so

257
00:11:20,040 --> 00:11:21,899
it's going to store both the embeddics

258
00:11:21,899 --> 00:11:24,000
that it computed as well as the original

259
00:11:24,000 --> 00:11:26,160
chunks from the documents

260
00:11:26,160 --> 00:11:28,740
now during the retrieval process the llm

261
00:11:28,740 --> 00:11:30,540
is going to have access to both the

262
00:11:30,540 --> 00:11:33,660
embeddings as well as the chunks that

263
00:11:33,660 --> 00:11:36,660
are retrieved based on the embedding

264
00:11:36,660 --> 00:11:39,240
model I hope this is clear now let's

265
00:11:39,240 --> 00:11:41,160
look at some of the customizations that

266
00:11:41,160 --> 00:11:43,200
you can make the first customization

267
00:11:43,200 --> 00:11:45,420
that we are going to be looking at is

268
00:11:45,420 --> 00:11:48,060
how to change the llm now in order to

269
00:11:48,060 --> 00:11:50,100
change the default values we are going

270
00:11:50,100 --> 00:11:52,140
to be using the concept of service

271
00:11:52,140 --> 00:11:55,440
context so this defines all the default

272
00:11:55,440 --> 00:11:58,260
values of different parameters within

273
00:11:58,260 --> 00:11:59,579
lamba index

274
00:11:59,579 --> 00:12:03,420
Now by default I think that llm lamb

275
00:12:03,420 --> 00:12:06,600
index uses is The DaVinci zero three

276
00:12:06,600 --> 00:12:08,880
model however if you want to change it

277
00:12:08,880 --> 00:12:12,240
to let's say GPT 3.5 turbo so we

278
00:12:12,240 --> 00:12:14,760
basically use the open AI function

279
00:12:14,760 --> 00:12:18,000
within in Lamb index create an llm based

280
00:12:18,000 --> 00:12:19,320
on that so you can Define the

281
00:12:19,320 --> 00:12:21,240
temperature the max number of tokens

282
00:12:21,240 --> 00:12:23,700
that is going to generate right and then

283
00:12:23,700 --> 00:12:25,440
ask that llm

284
00:12:25,440 --> 00:12:28,680
to this service context to change the

285
00:12:28,680 --> 00:12:31,140
defaults so here we're changing the

286
00:12:31,140 --> 00:12:36,600
default llm to GPT 3.5 and then ask that

287
00:12:36,600 --> 00:12:39,899
Ellen to the vector store index that we

288
00:12:39,899 --> 00:12:41,339
just created

289
00:12:41,339 --> 00:12:43,740
and then you can use the vector store

290
00:12:43,740 --> 00:12:47,100
index exactly the way we used above now

291
00:12:47,100 --> 00:12:49,440
if you want to use another llm such as

292
00:12:49,440 --> 00:12:52,079
let's say Google Palm so again you can

293
00:12:52,079 --> 00:12:56,100
simply import the Palm llm from gamma

294
00:12:56,100 --> 00:12:59,279
index llm class then pass that from

295
00:12:59,279 --> 00:13:01,920
added in to the edit and within the

296
00:13:01,920 --> 00:13:04,139
service context that will basically

297
00:13:04,139 --> 00:13:06,540
change the default value and then again

298
00:13:06,540 --> 00:13:10,500
you can pass it exactly like here to the

299
00:13:10,500 --> 00:13:12,779
vector store index and then it will

300
00:13:12,779 --> 00:13:15,540
start using the Palm llm in this case

301
00:13:15,540 --> 00:13:19,019
again you will need to set the API key

302
00:13:19,019 --> 00:13:22,139
for pump but the process is going to be

303
00:13:22,139 --> 00:13:24,420
very similar to what we did for open AI

304
00:13:24,420 --> 00:13:26,639
model now another parameter which is

305
00:13:26,639 --> 00:13:29,519
very important when it comes to these

306
00:13:29,519 --> 00:13:31,800
chat Bots for your documents is the

307
00:13:31,800 --> 00:13:34,139
chunk size so in order to change the

308
00:13:34,139 --> 00:13:36,240
channel size we can follow the same

309
00:13:36,240 --> 00:13:39,899
pattern so here you can simply pass on

310
00:13:39,899 --> 00:13:42,899
and Define two other parameters so the

311
00:13:42,899 --> 00:13:45,060
first is the check size the second was

312
00:13:45,060 --> 00:13:48,899
one is chunk overlap so here I'm using a

313
00:13:48,899 --> 00:13:51,180
thousand tokens for example with an

314
00:13:51,180 --> 00:13:54,060
overlap of 20 tokens

315
00:13:54,060 --> 00:13:56,459
so I would recommend everybody to look

316
00:13:56,459 --> 00:13:58,620
at the documentations and see what the

317
00:13:58,620 --> 00:14:00,839
default values are and how you can

318
00:14:00,839 --> 00:14:03,839
change them in here now another way of

319
00:14:03,839 --> 00:14:06,440
doing this is you can set the global

320
00:14:06,440 --> 00:14:09,180
Service context so in that case you

321
00:14:09,180 --> 00:14:12,480
don't have to pass it to the vector

322
00:14:12,480 --> 00:14:14,760
store index that you create you simply

323
00:14:14,760 --> 00:14:18,000
set the global Service contract using

324
00:14:18,000 --> 00:14:19,560
the service context that you just

325
00:14:19,560 --> 00:14:22,380
created and then you can use the those

326
00:14:22,380 --> 00:14:25,260
as defaults in the rest of your code now

327
00:14:25,260 --> 00:14:27,120
in this last example I'll show you how

328
00:14:27,120 --> 00:14:29,940
to use an open source llm from hugging

329
00:14:29,940 --> 00:14:32,820
face and the way you do it is we we're

330
00:14:32,820 --> 00:14:34,880
importing the hearing phase Ln class

331
00:14:34,880 --> 00:14:37,980
mlama index llms then you can also

332
00:14:37,980 --> 00:14:40,800
import prompt template from the prompts

333
00:14:40,800 --> 00:14:43,440
class this is very similar to what you

334
00:14:43,440 --> 00:14:46,199
probably have seen in that chain right

335
00:14:46,199 --> 00:14:48,480
so here's basically the system prompt

336
00:14:48,480 --> 00:14:51,120
because some of the open source elements

337
00:14:51,120 --> 00:14:53,699
need a system prompt but this system

338
00:14:53,699 --> 00:14:56,459
prompt is specifically for the stable LM

339
00:14:56,459 --> 00:15:00,240
models in close the user query within

340
00:15:00,240 --> 00:15:02,579
some special tokens right now in order

341
00:15:02,579 --> 00:15:04,399
to use the llm

342
00:15:04,399 --> 00:15:07,860
it's basically using lamba CPP in the

343
00:15:07,860 --> 00:15:12,240
background so we create an llm based on

344
00:15:12,240 --> 00:15:13,860
different parameters so for example the

345
00:15:13,860 --> 00:15:15,839
context window you can pass this on

346
00:15:15,839 --> 00:15:17,940
maximum new tokens that you want to

347
00:15:17,940 --> 00:15:20,519
generate right and other parameters such

348
00:15:20,519 --> 00:15:23,160
as the temperature that you want to set

349
00:15:23,160 --> 00:15:25,380
you want to do sampling or not right

350
00:15:25,380 --> 00:15:28,500
then you need to pass the model name as

351
00:15:28,500 --> 00:15:31,380
well as the token as a name this system

352
00:15:31,380 --> 00:15:34,260
prompt if the model supports one then

353
00:15:34,260 --> 00:15:37,019
the prompt template with the create user

354
00:15:37,019 --> 00:15:40,160
query right if you have multiple

355
00:15:40,160 --> 00:15:43,019
gpus and you want to all use all of them

356
00:15:43,019 --> 00:15:46,980
you can set the device map to Auto now

357
00:15:46,980 --> 00:15:50,100
in some certain cases there are certain

358
00:15:50,100 --> 00:15:52,279
specific tokens

359
00:15:52,279 --> 00:15:55,199
that are going to be used as stopping

360
00:15:55,199 --> 00:15:57,839
IDs you can pass on those depending on

361
00:15:57,839 --> 00:15:59,519
the model that you're using because

362
00:15:59,519 --> 00:16:01,139
different models are going to be using

363
00:16:01,139 --> 00:16:04,320
different tokenizes right so these are

364
00:16:04,320 --> 00:16:05,519
different parameters you can set

365
00:16:05,519 --> 00:16:08,399
essentially this will create an llm

366
00:16:08,399 --> 00:16:11,760
class now you can simply pass that

367
00:16:11,760 --> 00:16:14,279
editing class to the service context

368
00:16:14,279 --> 00:16:16,800
that we created right along with the

369
00:16:16,800 --> 00:16:19,440
setup chunk size that we want to use

370
00:16:19,440 --> 00:16:22,620
this will set the default element to

371
00:16:22,620 --> 00:16:26,120
this hugging face llm and then you can

372
00:16:26,120 --> 00:16:31,620
do query on your web restore and this

373
00:16:31,620 --> 00:16:33,000
alert them is going to be used for

374
00:16:33,000 --> 00:16:36,000
generating the responses now this was a

375
00:16:36,000 --> 00:16:37,860
quick overview of how to build a

376
00:16:37,860 --> 00:16:40,500
document q a system you using gamma

377
00:16:40,500 --> 00:16:43,620
index now learner index is extremely

378
00:16:43,620 --> 00:16:46,079
powerful that it has some really cool

379
00:16:46,079 --> 00:16:48,120
features for example one of the

380
00:16:48,120 --> 00:16:49,920
applications that I'm currently looking

381
00:16:49,920 --> 00:16:52,980
at is how to fine-tune embedding models

382
00:16:52,980 --> 00:16:56,040
along with fine tuning the additives uh

383
00:16:56,040 --> 00:16:57,600
so this is going to be the first video

384
00:16:57,600 --> 00:17:00,120
in the series on the number index I will

385
00:17:00,120 --> 00:17:03,360
be creating more advanced tutorials and

386
00:17:03,360 --> 00:17:05,220
on the on different topics and different

387
00:17:05,220 --> 00:17:07,439
features that Rama index have if you

388
00:17:07,439 --> 00:17:09,000
like what you see on the channel and

389
00:17:09,000 --> 00:17:11,040
would like to support it check out my

390
00:17:11,040 --> 00:17:14,760
patreon now if you want to build a q a

391
00:17:14,760 --> 00:17:18,419
system and want to use LinkedIn instead

392
00:17:18,419 --> 00:17:21,120
of lava index I would recommend you to

393
00:17:21,120 --> 00:17:23,459
check out this video as always if you

394
00:17:23,459 --> 00:17:25,860
found the content helpful consider

395
00:17:25,860 --> 00:17:27,540
liking the video and subscribe to the

396
00:17:27,540 --> 00:17:30,240
channel thanks for watching and see you

397
00:17:30,240 --> 00:17:32,720
in the next one

