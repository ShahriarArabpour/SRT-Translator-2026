1
00:00:00,000 --> 00:00:01,920
‫امروز به شما نشان می‌دهم چطور‬

2
00:00:01,920 --> 00:00:05,040
‫یک سیستم چت با اسناد را فقط در‬

3
00:00:05,040 --> 00:00:08,340
‫چهار خط کد پیاده‌سازی کنید. من اخیراً شروع کردم‬

4
00:00:08,340 --> 00:00:11,700
‫به بررسی LlamaIndex که یک‬

5
00:00:11,700 --> 00:00:14,519
‫جایگزین برای LangChain است. درست مثل LangChain،‬

6
00:00:14,519 --> 00:00:17,520
‫LlamaIndex این قابلیت را به ما می‌دهد که‬

7
00:00:17,520 --> 00:00:19,500
‫اپلیکیشن‌های قدرتمندی بر اساس‬

8
00:00:19,500 --> 00:00:22,080
‫مدل‌های زبان بزرگ بسازیم. این اپلیکیشن‌ها‬

9
00:00:22,080 --> 00:00:24,180
‫شامل پرسش و پاسخ از اسناد،‬

10
00:00:24,180 --> 00:00:27,060
‫چت‌بات‌های تقویت‌شده با داده و ایجنت‌های دانش هستند.‬

11
00:00:27,060 --> 00:00:28,800
‫بخش عالی ماجرا این است که می‌توانید‬

12
00:00:28,800 --> 00:00:30,660
‫انواع مختلفی از منابع داده را متصل کنید،‬

13
00:00:30,660 --> 00:00:33,600
‫شامل داده‌های ساختاریافته، بدون ساختار و‬

14
00:00:33,600 --> 00:00:36,360
‫نیمه‌ساختاریافته. بنابراین با استفاده از‬

15
00:00:36,360 --> 00:00:38,940
‫LlamaIndex می‌توانید به سرعت اپلیکیشن‌های مبتنی بر LLM‬

16
00:00:38,940 --> 00:00:41,579
‫بسازید، درست مثل LangChain.‬

17
00:00:41,579 --> 00:00:44,399
‫دلیل اصلی من برای بررسی LlamaIndex‬

18
00:00:44,399 --> 00:00:46,980
‫قابلیت fine-tune کردن‬

19
00:00:46,980 --> 00:00:49,140
‫مدل‌های embedding برای بهبود‬

20
00:00:49,140 --> 00:00:51,660
‫عملکرد سیستم‌های پرسش و پاسخ از اسناد بود.‬

21
00:00:51,660 --> 00:00:54,360
‫با استفاده از مدل‌های زبان بزرگ. اساساً‬

22
00:00:54,360 --> 00:00:56,699
‫این یعنی شما نه تنها می‌توانید‬

23
00:00:56,699 --> 00:00:58,559
‫مدل زبان بزرگ را روی دیتای خودتان‬

24
00:00:58,559 --> 00:01:01,079
‫فاین‌تیون (fine-tune) کنید، بلکه می‌توانید‬

25
00:01:01,079 --> 00:01:03,480
‫مدل embedding را هم فاین‌تیون کنید و این کار‬

26
00:01:03,480 --> 00:01:05,400
‫عملکرد سیستم بازیابی (retrieval) شما را بهبود می‌بخشد.‬

27
00:01:05,400 --> 00:01:08,880
‫من این موضوع را در یک ویدیوی آینده پوشش خواهم داد.‬

28
00:01:08,880 --> 00:01:11,820
‫در این ویدیوی اول، به شما نشان می‌دهم که چطور‬

29
00:01:11,820 --> 00:01:13,979
‫یک سیستم چت با اسناد خودتان بسازید‬

30
00:01:13,979 --> 00:01:17,220
‫با استفاده از LlamaIndex. احتمالاً قبلاً‬

31
00:01:17,220 --> 00:01:19,200
‫این دیاگرام را دیده‌اید. خب، اجزای مختلفی‬

32
00:01:19,200 --> 00:01:21,659
‫وجود دارد. اولین جزء‬

33
00:01:21,659 --> 00:01:25,320
‫اساساً بارگذاری اسناد شماست. سپس‬

34
00:01:25,320 --> 00:01:27,540
‫شما می‌خواهید اسنادتان را به‬

35
00:01:27,540 --> 00:01:30,659
‫تکه‌های (chunk) کوچکتر با یک chunk از پیش تعریف‌شده‬

36
00:01:30,659 --> 00:01:33,479
‫تقسیم کنید. برای هر کدام از این chunkها شما‬

37
00:01:33,479 --> 00:01:36,000
‫امبدینگ‌های (embedding) متفاوتی را محاسبه می‌کنید. این embeddingها‬

38
00:01:36,000 --> 00:01:38,579
‫نمایش‌های عددی از‬

39
00:01:38,579 --> 00:01:41,340
‫متن موجود در یک chunk هستند. بعد شما‬

40
00:01:41,340 --> 00:01:45,000
‫یک ایندکس معنایی (semantic index) ایجاد می‌کنید. اساساً‬

41
00:01:45,000 --> 00:01:48,119
‫این Vector Store شماست. بعد از اون،‬

42
00:01:48,119 --> 00:01:50,460
‫می‌تونید واقعاً از این Vector Store برای‬

43
00:01:50,460 --> 00:01:53,159
‫چت کردن با اسنادتون استفاده کنید. حالا برای اینکه‬

44
00:01:53,159 --> 00:01:54,840
‫با سندتون چت کنید،‬

45
00:01:54,840 --> 00:01:57,540
‫سیستم سوال شما رو می‌گیره و محاسبه می‌کنه‬

46
00:01:57,540 --> 00:01:59,939
‫همون Embeddingای رو که برای‬

47
00:01:59,939 --> 00:02:02,640
‫چانک‌ها (chunks) محاسبه کرده بود. بعد یک‬

48
00:02:02,640 --> 00:02:04,439
‫جستجوی معنایی روی پایگاه دانش شما انجام میده.‬

49
00:02:04,439 --> 00:02:05,880
‫بنابراین،‬

50
00:02:05,880 --> 00:02:08,099
‫در نتیجه این جستجو،‬

51
00:02:08,099 --> 00:02:09,300
‫برمی‌گردونه‬

52
00:02:09,300 --> 00:02:11,280
‫تعداد مشخصی از چانک‌ها (chunks) رو که‬

53
00:02:11,280 --> 00:02:13,739
‫به سوال شما مرتبط هستن. مثلاً فرض کنیم‬

54
00:02:13,739 --> 00:02:16,140
‫به طور پیش‌فرض LlamaIndex فقط دو‬

55
00:02:16,140 --> 00:02:18,000
‫چانک (chunk) از بین تمام چانک‌های موجود‬

56
00:02:18,000 --> 00:02:20,400
‫در پایگاه دانش رو برمی‌گردونه.‬

57
00:02:20,400 --> 00:02:22,620
‫و این چانک‌های برگردونده شده به عنوان‬

58
00:02:22,620 --> 00:02:26,099
‫کانتکست (context) برای LLM منتخب شما استفاده میشن.‬

59
00:02:26,099 --> 00:02:29,040
‫بنابراین LLM سوال کاربر رو‬

60
00:02:29,040 --> 00:03:01,440
‫به همراه کانتکستی که برگردونده شده دریافت می‌کنه.‬

61
00:02:31,440 --> 00:02:33,780
‫توسط مدل Embedding در طول‬

62
00:02:33,780 --> 00:02:36,420
‫جستجوی معنایی و LLM‬

63
00:02:36,420 --> 00:02:38,099
‫یک پاسخ تولید خواهد کرد و این پاسخ‬

64
00:02:38,099 --> 00:02:41,099
‫به کاربر نمایش داده خواهد شد. همانطور که می‌بینید این‬

65
00:02:41,099 --> 00:02:43,200
‫کل فرآیند را می‌توان به چهار‬

66
00:02:43,200 --> 00:02:46,379
‫مرحله مختلف تقسیم کرد و من به شما نشان خواهم داد که چگونه‬

67
00:02:46,379 --> 00:02:48,959
‫این کل فرآیند را فقط با استفاده از‬

68
00:02:48,959 --> 00:02:52,140
‫چهار خط کد در LlamaIndex پیاده‌سازی کنید.‬

69
00:02:52,140 --> 00:02:54,540
‫خب، بیایید نگاهی به کد بیندازیم.‬

70
00:02:54,540 --> 00:02:57,840
‫من از Google Colab استفاده می‌کنم تا‬

71
00:02:57,840 --> 00:03:00,660
‫این نوت‌بوک و کد را اجرا کنم. خب اول‬

72
00:03:00,660 --> 00:03:02,700
‫تمام پکیج‌های مورد نیاز را نصب می‌کنیم.‬

73
00:03:02,700 --> 00:03:05,940
‫ما LlamaIndex و بعد OpenAI را نصب می‌کنیم.‬

74
00:03:05,940 --> 00:03:08,459
‫برای مثال اولیه، من‬

75
00:03:08,459 --> 00:03:11,340
‫به شما نشان خواهم داد که چگونه پرسش و پاسخ روی اسناد را‬

76
00:03:11,340 --> 00:03:14,879
‫با استفاده از LLM شرکت OpenAI و‬

77
00:03:14,879 --> 00:03:17,340
‫Embeddingهای OpenAI پیاده‌سازی کنید. اما همچنین به شما نشان خواهم داد که چگونه‬

78
00:03:17,340 --> 00:03:20,940
‫LLM شرکت OpenAI را با یک LLM اوپن‌سورس‬

79
00:03:20,940 --> 00:03:23,340
‫از Hugging Face جایگزین کنید.‬

80
00:03:23,340 --> 00:03:25,680
‫و برای این کار به Transformers نیاز خواهیم داشت.‬

81
00:03:25,680 --> 00:03:28,280
‫پکیجی برای دسترسی به رابط Hugging Face‬

82
00:03:28,280 --> 00:03:32,159
‫برای LLMهای مبتنی بر آن و پکیج accelerate‬

83
00:03:32,159 --> 00:03:34,379
‫تا پردازش یا به عبارتی‬

84
00:03:34,379 --> 00:03:37,140
‫سرعت اجرای LLMهای محلی را بالا ببریم.‬

85
00:03:37,140 --> 00:03:40,080
‫خب، برای مثال اولیه ما نیاز خواهیم داشت‬

86
00:03:40,080 --> 00:03:43,260
‫به کلید API شرکت OpenAI تا بتوانیم دسترسی پیدا کنیم‬

87
00:03:43,260 --> 00:03:46,379
‫به Embeddingها و LLM آن، بنابراین شما‬

88
00:03:46,379 --> 00:03:48,000
‫باید آن را فراهم کنید.‬

89
00:03:48,000 --> 00:03:50,760
‫بسیار خب، در ادامه باید پکیج‌های مورد نیاز را import کنیم.‬

90
00:03:50,760 --> 00:03:53,280
‫پس ما OpenAI را import می‌کنیم‬

91
00:03:53,280 --> 00:03:56,099
‫از LlamaIndex. سپس دو تابع کمکی‬

92
00:03:56,099 --> 00:03:58,920
‫یا object را import می‌کنیم.‬

93
00:03:58,920 --> 00:04:01,980
‫یکی VectorStoreIndex است‬

94
00:04:01,980 --> 00:04:04,799
‫و دیگری SimpleDirectoryReader.‬

95
00:04:04,799 --> 00:04:08,159
‫کاربردشان وقتی به کد نگاه کنیم واضح‌تر می‌شود.‬

96
00:04:08,159 --> 00:04:10,860
‫و بعد از آن هم داریم چند پکیج دیگر را‬

97
00:04:10,860 --> 00:04:15,480
‫import می‌کنیم تا به درستی خروجی‬

98
00:04:15,480 --> 00:04:18,780
‫مدل را فرمت‌بندی کنیم. حالا برای اینکه‬

99
00:04:18,780 --> 00:04:21,299
‫با اسناد خود چت کنید،‬

100
00:04:21,299 --> 00:04:22,800
‫ابتدا باید به آن‌ها دسترسی داشته باشید.‬

101
00:04:22,800 --> 00:04:25,139
‫داکیومنت‌ها. خب، روشی که من این کار رو انجام میدم‬

102
00:04:25,139 --> 00:04:28,020
‫اینه که یک فولدر به اسم Data ایجاد می‌کنم‬

103
00:04:28,020 --> 00:04:29,940
‫و داخل این فولدر آپلود می‌کنم‬

104
00:04:29,940 --> 00:04:34,440
‫این فایل رو. حالا این مقاله مشخص‬

105
00:04:34,440 --> 00:04:37,020
‫عنوانش هست «روی چه چیزی کار کردم» و این‬

106
00:04:37,020 --> 00:04:39,120
‫در واقع مقاله‌ایه که توسط پال گراهام نوشته شده‬

107
00:04:39,120 --> 00:04:42,720
‫زمانی که با Y Combinator کار می‌کرد.‬

108
00:04:42,720 --> 00:04:45,540
‫پس ما قراره از این به عنوان یک‬

109
00:04:45,540 --> 00:04:48,660
‫دیتاست نمونه استفاده کنیم، اما شما می‌تونید هر نوع‬

110
00:04:48,660 --> 00:04:51,720
‫داکیومنتی رو آپلود کنید، مثلا فایل‌های متنی، Word‬

111
00:04:51,720 --> 00:04:55,259
‫یا PDF. حالا برگردیم به‬

112
00:04:55,259 --> 00:04:57,780
‫معماری. اول باید داکیومنت‌ها رو لود کنیم.‬

113
00:04:57,780 --> 00:05:00,840
‫حالا LlamaIndex یک قابلیت خیلی ساده‬

114
00:05:00,840 --> 00:05:03,240
‫برای این کار فراهم می‌کنه. بنابراین‬

115
00:05:03,240 --> 00:05:04,979
‫یک کلاسی به اسم Simple‬

116
00:05:04,979 --> 00:05:07,020
‫DirectoryReader وجود داره که شما فقط باید‬

117
00:05:07,020 --> 00:05:10,380
‫اسم فولدر یا مسیر اون‬

118
00:05:10,380 --> 00:05:13,080
‫فولدر رو بهش بدید. در این مورد، فولدر ما‬

119
00:05:13,080 --> 00:05:15,120
‫اسمش data هست و برای همین من اسم‬

120
00:05:15,120 --> 00:05:17,580
‫data رو وارد کردم. بعدش باید فراخوانی کنیم‬

121
00:05:17,580 --> 00:05:20,100
‫تابع load_data را روی آن اجرا می‌کنیم و‬

122
00:05:20,100 --> 00:05:22,500
‫این کار اسناد ما را بارگذاری می‌کند.‬

123
00:05:22,500 --> 00:05:24,960
‫حالا نکته عالی در مورد این موضوع این است که شما‬

124
00:05:24,960 --> 00:05:26,759
‫می‌توانید انواع مختلفی از اسناد را‬

125
00:05:26,759 --> 00:05:29,940
‫درون پوشه داشته باشید و این کلاس مشخص خواهد کرد‬

126
00:05:29,940 --> 00:05:32,580
‫که برای آن اسناد از کدام loader استفاده کند.‬

127
00:05:32,580 --> 00:05:33,720
‫اسناد.‬

128
00:05:33,720 --> 00:05:36,180
‫و می‌توانیم به آن هم نگاهی بیندازیم. این‬

129
00:05:36,180 --> 00:05:38,400
‫اساساً همان فایل متنی است و کل‬

130
00:05:38,400 --> 00:05:40,680
‫فایل متنی در اینجا بارگذاری شده است.‬

131
00:05:40,680 --> 00:05:43,560
‫خب، قدم بعدی در معماری ما این است که‬

132
00:05:43,560 --> 00:05:45,900
‫یک vector store ایجاد کنیم. این کار‬

133
00:05:45,900 --> 00:05:48,180
‫شامل تقسیم کردن اسناد شما به‬

134
00:05:48,180 --> 00:05:51,000
‫تکه‌ها (chunks)، محاسبه embeddingهای آنها و بعد‬

135
00:05:51,000 --> 00:05:53,520
‫ذخیره کردن هم embeddingها و هم‬

136
00:05:53,520 --> 00:05:55,440
‫آن chunkها در یک vector store است.‬

137
00:05:55,440 --> 00:05:58,080
‫باز هم، LlamaIndex این کار را بسیار‬

138
00:05:58,080 --> 00:06:00,600
‫آسان می‌کند. ما از کلاس VectorStoreIndex‬

139
00:06:00,600 --> 00:06:03,300
‫استفاده خواهیم کرد. سپس‬

140
00:06:03,300 --> 00:06:05,460
‫vector store را از روی اسناد ایجاد می‌کنیم.‬

141
00:06:05,460 --> 00:06:08,699
‫ما اسنادمون رو اینجا بهش پاس می‌دیم و‬

142
00:06:08,699 --> 00:06:12,240
‫این یک ایندکس Vector Store برامون‬

143
00:06:12,240 --> 00:06:13,259
‫ایجاد می‌کنه.‬

144
00:06:13,259 --> 00:06:16,320
‫خب، پس ما الان ایندکسمون رو ساختیم.‬

145
00:06:16,320 --> 00:06:18,360
‫در ادامه می‌خوایم ببینیم چطور‬

146
00:06:18,360 --> 00:06:20,660
‫گزینه‌های مختلف رو سفارشی‌سازی کنیم، برای مثال‬

147
00:06:20,660 --> 00:06:23,280
‫اندازه chunk، نوع embeddingها‬

148
00:06:23,280 --> 00:06:26,039
‫و همه چیز. اما قبل از اون، فقط‬

149
00:06:26,039 --> 00:06:28,680
‫می‌خوایم سوال بپرسیم. برای این کار‬

150
00:06:28,680 --> 00:06:31,440
‫باید یک query engine بسازیم.‬

151
00:06:31,440 --> 00:06:34,699
‫به نظرم این فرآیند خیلی ساده و روانه‬

152
00:06:34,699 --> 00:06:38,479
‫در LlamaIndex در مقایسه با...‬

153
00:06:38,479 --> 00:06:41,900
‫خب اینجا اساساً شما می‌تونید‬

154
00:06:41,900 --> 00:06:45,840
‫یک query engine روی ایندکس بسازید با‬

155
00:06:45,840 --> 00:06:49,259
‫استفاده از متد `as_query_engine`.‬

156
00:06:49,259 --> 00:06:51,539
‫اگه بخواید با اسنادتون چت کنید‬

157
00:06:51,539 --> 00:06:54,900
‫و بخواید حافظه داشته باشید،‬

158
00:06:54,900 --> 00:06:57,080
‫باید از یک تابعی به اسم‬

159
00:06:57,080 --> 00:07:00,000
‫`as_chat_engine` استفاده کنید و این اساساً‬

160
00:07:00,000 --> 00:07:03,120
‫حافظه رو فعال می‌کنه. اما `as_query_engine` حافظه نداره.‬

161
00:07:03,120 --> 00:07:05,819
‫مولفه حافظه را اینجا داریم. حالا‬

162
00:07:05,819 --> 00:07:08,880
‫برای اینکه از این query engine استفاده کنیم تا یک‬

163
00:07:08,880 --> 00:07:11,280
‫پاسخ بر اساس سوال کاربر بگیریم،‬

164
00:07:11,280 --> 00:07:12,900
‫باید این فرآیند را طی کند که‬

165
00:07:12,900 --> 00:07:14,699
‫باید Embeddingها را برای سوال محاسبه کند،‬

166
00:07:14,699 --> 00:07:17,280
‫سپس جستجوی معنایی را انجام دهد و‬

167
00:07:17,280 --> 00:07:19,979
‫از چانک‌های بازگشتی به عنوان کانتکست به همراه‬

168
00:07:19,979 --> 00:07:21,960
‫LLM و سوال استفاده کند تا‬

169
00:07:21,960 --> 00:07:24,780
‫یک پاسخ تولید کند. همه اینها‬

170
00:07:24,780 --> 00:07:27,240
‫با استفاده از یک خط کد پیاده‌سازی شده است.‬

171
00:07:27,240 --> 00:07:30,300
‫اما اساساً کاری که می‌کنید این است که از‬

172
00:07:30,300 --> 00:07:32,639
‫query engine استفاده کرده و سپس این تابع‬

173
00:07:32,639 --> 00:07:35,940
‫به نام query را فراخوانی می‌کنید و سوال خود را‬

174
00:07:35,940 --> 00:07:38,759
‫به این تابع پاس می‌دهید. برای مثال،‬

175
00:07:38,759 --> 00:07:41,280
‫سوالی که می‌پرسیم این است که «نویسنده‬

176
00:07:41,280 --> 00:07:44,699
‫در دوران رشد چه کار می‌کرد؟» و ما‬

177
00:07:44,699 --> 00:07:47,580
‫اینجا یک پاسخ دریافت خواهیم کرد. حالا پاسخی‬

178
00:07:47,580 --> 00:07:49,259
‫که دریافت می‌کنید مولفه‌های مختلف‬

179
00:07:49,259 --> 00:07:51,960
‫زیادی دارد، اما ما صرفاً‬

180
00:07:51,960 --> 00:07:54,180
‫به خود جواب علاقه‌مند هستیم.‬

181
00:07:54,180 --> 00:07:57,300
‫کمی بعد بهتون نشون میدم معنی تمام این متن چیه‬

182
00:07:57,300 --> 00:08:00,120
‫اما اول اجازه بدید نشون بدم چطور‬

183
00:08:00,120 --> 00:08:03,120
‫جواب رو از LLM بگیریم. برای این کار،‬

184
00:08:03,120 --> 00:08:05,180
‫از تابع display مربوط به‬

185
00:08:05,180 --> 00:08:08,819
‫IPython استفاده می‌کنیم و پاسخ رو در قالب Markdown قرار میدیم‬

186
00:08:08,819 --> 00:08:11,220
‫و اون رو bold می‌کنیم. خب این هم از جواب:‬

187
00:08:11,220 --> 00:08:13,440
‫از طرف مدل: نویسنده روی‬

188
00:08:13,440 --> 00:08:15,120
‫نوشتن و برنامه‌نویسی خارج از‬

189
00:08:15,120 --> 00:08:17,520
‫مدرسه و قبل از کالج کار می‌کرد. او داستان‌های کوتاه می‌نوشت‬

190
00:08:17,520 --> 00:08:20,520
‫و سعی می‌کرد روی یک کامپیوتر‬

191
00:08:20,520 --> 00:08:23,819
‫IBM 1401 با استفاده از یک نسخه اولیه‬

192
00:08:23,819 --> 00:08:26,039
‫از Fortran برنامه بنویسد. خب، این همان جوابی است‬

193
00:08:26,039 --> 00:08:29,400
‫که توسط LLM با استفاده از کانتکستی‬

194
00:08:29,400 --> 00:08:32,000
‫که مدل embedding فراهم کرده بود، تولید شد.‬

195
00:08:32,000 --> 00:08:34,459
‫همانطور که می‌بینید،‬

196
00:08:34,459 --> 00:08:37,680
‫به غیر از importها، ما توانستیم‬

197
00:08:37,680 --> 00:08:41,279
‫یک سیستم پرسش و پاسخ اسناد را فقط با‬

198
00:08:41,279 --> 00:08:42,839
‫همین چند خط کد بسازیم.‬

199
00:08:42,839 --> 00:08:46,320
‫و این پیاده‌سازی خیلی تمیز است.‬

200
00:08:46,320 --> 00:08:48,899
‫در ادامه، می‌خواهم به شما نشان دهم که چطور شخصی‌سازی کنید.‬

201
00:08:48,899 --> 00:08:51,300
‫بخش‌های مختلف این دیاگرام، برای‬

202
00:08:51,300 --> 00:08:53,700
‫مثال اینکه چطور `chunk size` رو تعریف می‌کنید،‬

203
00:08:53,700 --> 00:08:56,580
‫یا `chunk overlap`، یا اینکه چطور‬

204
00:08:56,580 --> 00:08:59,160
‫`LLM` رو تغییر میدید و غیره. اما قبل از‬

205
00:08:59,160 --> 00:09:01,500
‫اون، بیایید به چند مفهوم خیلی مهم‬

206
00:09:01,500 --> 00:09:03,480
‫نگاهی بندازیم وقتی صحبت از‬

207
00:09:03,480 --> 00:09:06,240
‫`Vector Store`ها میشه. خب در حال حاضر ایندکسی که ما‬

208
00:09:06,240 --> 00:09:09,779
‫ایجاد کردیم فقط در حافظه هست اما شما می‌تونید‬

209
00:09:09,779 --> 00:09:12,420
‫با فراخوانی این تابع اون رو پایدار (persist) کنید‬

210
00:09:12,420 --> 00:09:15,420
‫یعنی تابع `persist` روی ایندکسی که ایجاد کردید.‬

211
00:09:15,420 --> 00:09:18,300
‫خب، نحوه کارش اینطوریه که اگه ما‬

212
00:09:18,300 --> 00:09:21,240
‫این رو اجرا کنیم، به طور پیش‌فرض قراره‬

213
00:09:21,240 --> 00:09:24,240
‫یک پوشه به نام `storage` ایجاد کنه و‬

214
00:09:24,240 --> 00:09:27,180
‫داخل این پوشه چندین فایل `JSON`‬

215
00:09:27,180 --> 00:09:30,360
‫خواهید دید. حالا این قابلیت خیلی حیاتیه‬

216
00:09:30,360 --> 00:09:32,880
‫چون به شما این امکان رو میده که‬

217
00:09:32,880 --> 00:09:33,899
‫ایندکس رو یک بار ایجاد کنید،‬

218
00:09:33,899 --> 00:09:36,440
‫اون رو روی دیسک‌تون ذخیره کنید‬

219
00:09:36,440 --> 00:09:41,160
‫و بعد در اجراهای آینده ازش استفاده کنید.‬

220
00:09:41,160 --> 00:09:43,680
‫خب، حالا برای اینکه یک `Vector Store` رو بارگذاری کنیم‬

221
00:09:43,680 --> 00:09:46,440
‫ایندکسی که روی دیسک ذخیره شده بود، ما‬

222
00:09:46,440 --> 00:09:48,480
‫از `storage context` استفاده خواهیم کرد‬

223
00:09:48,480 --> 00:09:51,660
‫به همراه کلاس `load_index_from_storage`.‬

224
00:09:51,660 --> 00:09:53,279
‫‬

225
00:09:53,279 --> 00:09:55,320
‫خب، روش کار به این صورت است که شما‬

226
00:09:55,320 --> 00:09:58,320
‫از یک `storage context` برای خواندن‬

227
00:09:58,320 --> 00:10:01,680
‫محتوای ایندکس استفاده می‌کنید و بعد از‬

228
00:10:01,680 --> 00:10:03,420
‫همون `storage context` که ایجاد کردید،‬

229
00:10:03,420 --> 00:10:06,120
‫ایندکس رو بازسازی می‌کنید.‬

230
00:10:06,120 --> 00:10:08,459
‫درسته؟ و بعد می‌تونید دقیقاً‬

231
00:10:08,459 --> 00:10:10,680
‫مثل قبل ازش استفاده کنید.‬

232
00:10:10,680 --> 00:10:13,019
‫خب امیدوارم این واضح باشه. سوال دیگه‌ای‬

233
00:10:13,019 --> 00:10:16,200
‫که به ذهن میاد اینه که دقیقاً چه چیزی‬

234
00:10:16,200 --> 00:10:18,660
‫داخل این `Vector store` وجود داره؟‬

235
00:10:18,660 --> 00:10:20,640
‫خب برای اینکه بهتون نشون بدم، من می‌خوام‬

236
00:10:20,640 --> 00:10:23,880
‫هر چهار فایل رو باز کنم و دو تا‬

237
00:10:23,880 --> 00:10:25,500
‫فایل مهمی که می‌خوایم بهشون نگاه کنیم‬

238
00:10:25,500 --> 00:10:28,980
‫فایل‌های `docstore` و `vector_store` هستن.‬

239
00:10:28,980 --> 00:10:31,500
‫خب `vector store` همونطور که اینجا می‌بینید‬

240
00:10:31,500 --> 00:10:33,720
‫در واقع همون `Embedding`هایی هست که ما...‬

241
00:10:33,720 --> 00:10:37,019
‫برای هر chunk محاسبه میشه و‬

242
00:10:37,019 --> 00:10:39,720
‫بسته به مدل Embedding که‬

243
00:10:39,720 --> 00:10:41,640
‫انتخاب می‌کنید، تعداد متفاوتی‬

244
00:10:41,640 --> 00:10:44,579
‫Embedding برای هر chunk دریافت خواهید کرد.‬

245
00:10:44,579 --> 00:10:46,680
‫خب، مشابه Vector Store برای‬

246
00:10:46,680 --> 00:10:49,920
‫Embeddingها، یک doc store هم وجود داره. پس اینجا‬

247
00:10:49,920 --> 00:10:53,040
‫شما chunkهای مختلفی از‬

248
00:10:53,040 --> 00:10:55,440
‫سند رو می‌بینید. اساساً این متنیه که‬

249
00:10:55,440 --> 00:10:58,140
‫در chunkهای مختلف ذخیره شده و‬

250
00:10:58,140 --> 00:11:01,140
‫سومین چیز مهم، index store هست.‬

251
00:11:01,140 --> 00:11:04,620
‫اساساً این شامل هش یا‬

252
00:11:04,620 --> 00:11:08,279
‫آدرس برای chunkهای مختلفه و با استفاده از‬

253
00:11:08,279 --> 00:11:11,519
‫این index store مشخص می‌کنه که کدوم‬

254
00:11:11,519 --> 00:11:14,579
‫Embeddingها در اینجا به کدوم‬

255
00:11:14,579 --> 00:11:15,959
‫chunk تعلق داره.‬

256
00:11:15,959 --> 00:11:20,040
‫حالا وقتی یک Vector Store ایجاد می‌کنید،‬

257
00:11:20,040 --> 00:11:21,899
‫هم Embeddingهایی که محاسبه کرده‬

258
00:11:21,899 --> 00:11:24,000
‫و هم chunkهای اصلی‬

259
00:11:24,000 --> 00:11:26,160
‫از اسناد رو ذخیره می‌کنه.‬

260
00:11:26,160 --> 00:11:28,740
‫حالا در طول فرآیند بازیابی، LLM‬

261
00:11:28,740 --> 00:11:30,540
‫هم به Embeddingها دسترسی خواهد داشت‬

262
00:11:30,540 --> 00:11:33,660
‫و هم به chunkهایی که‬

263
00:11:33,660 --> 00:11:36,660
‫بر اساس مدل embedding بازیابی می‌شوند.‬

264
00:11:36,660 --> 00:11:39,240
‫امیدوارم این واضح باشه. حالا بیایید‬

265
00:11:39,240 --> 00:11:41,160
‫به برخی از سفارشی‌سازی‌هایی که‬

266
00:11:41,160 --> 00:11:43,200
‫می‌توانید انجام دهید نگاهی بیندازیم. اولین سفارشی‌سازی‬

267
00:11:43,200 --> 00:11:45,420
‫که به آن خواهیم پرداخت این است‬

268
00:11:45,420 --> 00:11:48,060
‫که چطور LLM را تغییر دهیم. خب، برای اینکه‬

269
00:11:48,060 --> 00:11:50,100
‫مقادیر پیش‌فرض را تغییر دهیم، از‬

270
00:11:50,100 --> 00:11:52,140
‫مفهوم service context استفاده خواهیم کرد.‬

271
00:11:52,140 --> 00:11:55,440
‫این تمام مقادیر پیش‌فرض‬

272
00:11:55,440 --> 00:11:58,260
‫پارامترهای مختلف را در‬

273
00:11:58,260 --> 00:11:59,579
‫LlamaIndex تعریف می‌کند.‬

274
00:11:59,579 --> 00:12:03,420
‫خب به طور پیش‌فرض، فکر می‌کنم LLMای که LlamaIndex‬

275
00:12:03,420 --> 00:12:06,600
‫استفاده می‌کند، مدل DaVinci-003 است.‬

276
00:12:06,600 --> 00:12:08,880
‫اما اگر بخواهید آن را تغییر دهید‬

277
00:12:08,880 --> 00:12:12,240
‫به مثلاً GPT-3.5-turbo، خب ما‬

278
00:12:12,240 --> 00:12:14,760
‫اساساً از تابع OpenAI‬

279
00:12:14,760 --> 00:12:18,000
‫درون LlamaIndex استفاده می‌کنیم و یک LLM بر اساس‬

280
00:12:18,000 --> 00:12:19,320
‫آن می‌سازیم. پس می‌توانید‬

281
00:12:19,320 --> 00:12:21,240
‫temperature، حداکثر تعداد توکن‌ها‬

282
00:12:21,240 --> 00:12:23,700
‫که قراره تولید کنه، درسته؟ و بعد‬

283
00:12:23,700 --> 00:12:25,440
‫اون LLM رو پاس بدیم‬

284
00:12:25,440 --> 00:12:28,680
‫به این service_context تا مقادیر پیش‌فرض رو‬

285
00:12:28,680 --> 00:12:31,140
‫تغییر بدیم. پس اینجا داریم‬

286
00:12:31,140 --> 00:12:36,600
‫LLM پیش‌فرض رو به GPT-3.5 تغییر می‌دیم و بعد اون رو‬

287
00:12:36,600 --> 00:12:39,899
‫به vector store index که‬

288
00:12:39,899 --> 00:12:41,339
‫همین الان ساختیم، پاس می‌دیم.‬

289
00:12:41,339 --> 00:12:43,740
‫و بعد می‌تونید از vector store‬

290
00:12:43,740 --> 00:12:47,100
‫index دقیقاً همونطوری که بالا استفاده کردیم، استفاده کنید.‬

291
00:12:47,100 --> 00:12:49,440
‫حالا اگه بخواید از یه LLM دیگه مثل‬

292
00:12:49,440 --> 00:12:52,079
‫مثلاً Google Palm استفاده کنید، دوباره می‌تونید‬

293
00:12:52,079 --> 00:12:56,100
‫به سادگی Palm LLM رو از کلاس LLM در LlamaIndex‬

294
00:12:56,100 --> 00:12:59,279
‫ایمپورت کنید، بعد اون رو‬

295
00:12:59,279 --> 00:13:01,920
‫به service_context پاس بدید.‬

296
00:13:01,920 --> 00:13:04,139
‫این کار اساساً مقدار پیش‌فرض رو‬

297
00:13:04,139 --> 00:13:06,540
‫تغییر می‌ده و بعد دوباره‬

298
00:13:06,540 --> 00:13:10,500
‫می‌تونید اون رو دقیقاً مثل اینجا به‬

299
00:13:10,500 --> 00:13:12,779
‫vector store index پاس بدید و بعدش‬

300
00:13:12,779 --> 00:13:15,540
‫در این حالت، شروع به استفاده از Palm LLM می‌کنه.‬

301
00:13:15,540 --> 00:13:19,019
‫باز هم باید کلید API را تنظیم کنید‬

302
00:13:19,019 --> 00:13:22,139
‫برای PaLM، اما فرآیندش قرار است که‬

303
00:13:22,139 --> 00:13:24,420
‫بسیار شبیه به کاری باشد که برای OpenAI انجام دادیم.‬

304
00:13:24,420 --> 00:13:26,639
‫مدل. حالا یک پارامتر دیگر که‬

305
00:13:26,639 --> 00:13:29,519
‫در مورد این چت‌بات‌های مخصوص اسناد بسیار مهم است،‬

306
00:13:29,519 --> 00:13:31,800
‫پارامتر chunk size است.‬

307
00:13:31,800 --> 00:13:34,139
‫پس برای تغییر‬

308
00:13:34,139 --> 00:13:36,240
‫مقدار chunk size، می‌توانیم از همان الگوی قبلی‬

309
00:13:36,240 --> 00:13:39,899
‫پیروی کنیم. پس اینجا می‌توانید به سادگی‬

310
00:13:39,899 --> 00:13:42,899
‫دو پارامتر دیگر را ارسال و تعریف کنید. خب،‬

311
00:13:42,899 --> 00:13:45,060
‫اولی chunk size است و دومی‬

312
00:13:45,060 --> 00:13:48,899
‫chunk overlap است. من اینجا از‬

313
00:13:48,899 --> 00:13:51,180
‫مثلاً هزار توکن با یک همپوشانی‬

314
00:13:51,180 --> 00:13:54,060
‫۲۰ توکنی استفاده می‌کنم.‬

315
00:13:54,060 --> 00:13:56,459
‫بنابراین به همه توصیه می‌کنم که نگاهی‬

316
00:13:56,459 --> 00:13:58,620
‫به مستندات بیندازند و ببینند که‬

317
00:13:58,620 --> 00:14:00,839
‫مقادیر پیش‌فرض چه هستند و چطور می‌توانید‬

318
00:14:00,839 --> 00:14:03,839
‫آن‌ها را اینجا تغییر دهید. حالا، یک راه دیگر‬

319
00:14:03,839 --> 00:14:06,440
‫برای انجام این کار این است که می‌توانید global‬

320
00:14:06,440 --> 00:14:09,180
‫Service Context را تنظیم کنید. در آن صورت شما...‬

321
00:14:09,180 --> 00:14:12,480
‫لازم نیست آن را به Vector‬

322
00:14:12,480 --> 00:14:14,760
‫Store indexی که می‌سازید پاس بدهید. به سادگی‬

323
00:14:14,760 --> 00:14:18,000
‫ServiceContext گلوبال را با استفاده از‬

324
00:14:18,000 --> 00:14:19,560
‫همان ServiceContextی که الان‬

325
00:14:19,560 --> 00:14:22,380
‫ایجاد کردید تنظیم می‌کنید و بعد می‌توانید از آن‌ها‬

326
00:14:22,380 --> 00:14:25,260
‫به عنوان پیش‌فرض در بقیه کدتان استفاده کنید. حالا‬

327
00:14:25,260 --> 00:14:27,120
‫در این مثال آخر، به شما نشان می‌دهم چطور‬

328
00:14:27,120 --> 00:14:29,940
‫از یک LLM اوپن‌سورس از Hugging‬

329
00:14:29,940 --> 00:14:32,820
‫Face استفاده کنید. روش کار این است که ما‬

330
00:14:32,820 --> 00:14:34,880
‫کلاس HuggingFaceLLM را از‬

331
00:14:34,880 --> 00:14:37,980
‫llama_index.llms ایمپورت می‌کنیم. بعد می‌توانید‬

332
00:14:37,980 --> 00:14:40,800
‫PromptTemplate را هم از ماژول prompts‬

333
00:14:40,800 --> 00:14:43,440
‫ایمپورت کنید. این خیلی شبیه چیزی است که‬

334
00:14:43,440 --> 00:14:46,199
‫احتمالاً در LangChain دیده‌اید، درست است؟‬

335
00:14:46,199 --> 00:14:48,480
‫خب این در واقع system prompt است‬

336
00:14:48,480 --> 00:14:51,120
‫چون بعضی از LLMهای اوپن‌سورس‬

337
00:14:51,120 --> 00:14:53,699
‫به یک system prompt نیاز دارند، اما این system‬

338
00:14:53,699 --> 00:14:56,459
‫prompt به طور خاص برای مدل StableLM است‬

339
00:14:56,459 --> 00:15:00,240
‫که کوئری کاربر را بین‬

340
00:15:00,240 --> 00:15:02,579
‫چند توکن خاص قرار می‌دهد.‬

341
00:15:02,579 --> 00:15:04,399
‫برای استفاده از LLM‬

342
00:15:04,399 --> 00:15:07,860
‫در واقع از llama.cpp در پس‌زمینه استفاده می‌کنه‬

343
00:15:07,860 --> 00:15:12,240
‫پس ما یک LLM بر اساس‬

344
00:15:12,240 --> 00:15:13,860
‫پارامترهای مختلف می‌سازیم. برای مثال،‬

345
00:15:13,860 --> 00:15:15,839
‫می‌تونید context window رو بهش پاس بدید‬

346
00:15:15,839 --> 00:15:17,940
‫حداکثر توکن‌های جدیدی که می‌خواید‬

347
00:15:17,940 --> 00:15:20,519
‫تولید بشه، درسته؟ و پارامترهای دیگه‌ای مثل‬

348
00:15:20,519 --> 00:15:23,160
‫پارامتر temperature که می‌خواید تنظیم کنید‬

349
00:15:23,160 --> 00:15:25,380
‫اینکه می‌خواید sampling انجام بشه یا نه، درسته؟‬

350
00:15:25,380 --> 00:15:28,500
‫بعد باید اسم مدل رو پاس بدید‬

351
00:15:28,500 --> 00:15:31,380
‫و همچنین اسم tokenizer، و system prompt‬

352
00:15:31,380 --> 00:15:34,260
‫اگه مدل ازش پشتیبانی کنه، بعد‬

353
00:15:34,260 --> 00:15:37,019
‫قالب prompt رو با کوئری کاربر،‬

354
00:15:37,019 --> 00:15:40,160
‫درسته؟ اگه چندتا‬

355
00:15:40,160 --> 00:15:43,019
‫GPU دارید و می‌خواید از همه‌شون استفاده کنید‬

356
00:15:43,019 --> 00:15:46,980
‫می‌تونید device_map رو روی Auto تنظیم کنید. حالا‬

357
00:15:46,980 --> 00:15:50,100
‫در بعضی موارد خاص، توکن‌های‬

358
00:15:50,100 --> 00:15:52,279
‫مشخصی وجود دارن‬

359
00:15:52,279 --> 00:15:55,199
‫که به عنوان stopping ID استفاده میشن.‬

360
00:15:55,199 --> 00:15:57,839
‫می‌تونید اون‌ها رو هم بسته به نیاز پاس بدید.‬

361
00:15:57,839 --> 00:15:59,519
‫مدلی که دارید استفاده می‌کنید چون‬

362
00:15:59,519 --> 00:16:01,139
‫مدل‌های مختلف از توکنایزرهای‬

363
00:16:01,139 --> 00:16:04,320
‫متفاوتی استفاده می‌کنند. پس این‌ها‬

364
00:16:04,320 --> 00:16:05,519
‫پارامترهای مختلفی هستند که می‌تونید تنظیم کنید.‬

365
00:16:05,519 --> 00:16:08,399
‫در اصل، این یک کلاس `LLM` ایجاد می‌کنه.‬

366
00:16:08,399 --> 00:16:11,760
‫حالا می‌تونید به سادگی اون‬

367
00:16:11,760 --> 00:16:14,279
‫کلاس `LLM` رو به `service context`‬

368
00:16:14,279 --> 00:16:16,800
‫که ایجاد کردیم پاس بدید، به همراه‬

369
00:16:16,800 --> 00:16:19,440
‫اون `chunk size` که می‌خوایم استفاده کنیم.‬

370
00:16:19,440 --> 00:16:22,620
‫این کار `LLM` پیش‌فرض رو روی‬

371
00:16:22,620 --> 00:16:26,120
‫این `Hugging Face LLM` تنظیم می‌کنه و بعدش می‌تونید‬

372
00:16:26,120 --> 00:16:31,620
‫روی `Vector Store` خودتون کوئری بزنید و این‬

373
00:16:31,620 --> 00:16:33,000
‫`LLM` برای تولید پاسخ‌ها‬

374
00:16:33,000 --> 00:16:36,000
‫استفاده خواهد شد. خب این یک‬

375
00:16:36,000 --> 00:16:37,860
‫مرور سریع بر نحوه ساخت یک‬

376
00:16:37,860 --> 00:16:40,500
‫سیستم پرسش و پاسخ اسناد با استفاده از `LlamaIndex` بود.‬

377
00:16:40,500 --> 00:16:43,620
‫خب، `LlamaIndex` فوق‌العاده‬

378
00:16:43,620 --> 00:16:46,079
‫قدرتمنده و ویژگی‌های خیلی جالبی داره.‬

379
00:16:46,079 --> 00:16:48,120
‫برای مثال، یکی از‬

380
00:16:48,120 --> 00:16:49,920
‫کاربردهایی که من در حال حاضر دارم بررسی می‌کنم‬

381
00:16:49,920 --> 00:16:52,980
‫نحوه فاین-تیون کردن مدل‌های Embedding است‬

382
00:16:52,980 --> 00:16:56,040
‫به همراه فاین-تیون کردن adapterها.‬

383
00:16:56,040 --> 00:16:57,600
‫خب، این اولین ویدیو خواهد بود‬

384
00:16:57,600 --> 00:17:00,120
‫در سری آموزش‌های LlamaIndex. من‬

385
00:17:00,120 --> 00:17:03,360
‫آموزش‌های پیشرفته‌تری خواهم ساخت‬

386
00:17:03,360 --> 00:17:05,220
‫در مورد موضوعات و ویژگی‌های مختلفی‬

387
00:17:05,220 --> 00:17:07,439
‫که LlamaIndex دارد. اگر‬

388
00:17:07,439 --> 00:17:09,000
‫از محتوای کانال خوشتان می‌آید و‬

389
00:17:09,000 --> 00:17:11,040
‫دوست دارید از آن حمایت کنید، به‬

390
00:17:11,040 --> 00:17:14,760
‫Patreon من سر بزنید. حالا اگر می‌خواهید یک سیستم پرسش و پاسخ (Q&A)‬

391
00:17:14,760 --> 00:17:18,419
‫بسازید و بخواهید به جای LlamaIndex از LangChain استفاده کنید،‬

392
00:17:18,419 --> 00:17:21,120
‫توصیه می‌کنم که‬

393
00:17:21,120 --> 00:17:23,459
‫این ویدیو را ببینید. مثل همیشه، اگر‬

394
00:17:23,459 --> 00:17:25,860
‫این محتوا برایتان مفید بود، لطفاً‬

395
00:17:25,860 --> 00:17:27,540
‫ویدیو را لایک کنید و در کانال سابسکرایب کنید.‬

396
00:17:27,540 --> 00:17:30,240
‫ممنون از تماشای شما و شما را‬

397
00:17:30,240 --> 00:17:32,720
‫در ویدیوی بعدی می‌بینم.‬

